{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43eaad7f-f315-4f50-938a-fd94f07ca15a",
   "metadata": {},
   "source": [
    "# <B> SageMaker pileline with `MLflow` for Anormaly Detection based on AutoEncoder </B>\n",
    "* Container: codna_pytorch_p310\n",
    "* [Example codes](https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-mlflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e8867-4692-4d7c-8009-127c7125489b",
   "metadata": {},
   "source": [
    "## AutoReload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b26cf7-b510-4508-b86f-bd18e3e96a17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013479d-b181-48ed-8cd4-bfa68a82131f",
   "metadata": {},
   "source": [
    "## parameter store 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d5bbe6-5c31-40e0-be7b-0de1935e7fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from utils.ssm import parameter_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ffc18b-f73c-4bde-9436-93c378ed3175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strRegionName=boto3.Session().region_name\n",
    "pm = parameter_store(strRegionName)\n",
    "strPrefix = pm.get_params(key=\"PREFIX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842031c-56e5-4728-8411-669ae40bac99",
   "metadata": {},
   "source": [
    "## pramamters for tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee18fe5f-e302-4913-9d7e-534aae3ad98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strAccountId = pm.get_params(key=\"-\".join([strPrefix, \"ACCOUNT-ID\"]))\n",
    "strBucketName = pm.get_params(key=\"-\".join([strPrefix, \"BUCKET\"]))\n",
    "strExecutionRole = pm.get_params(key=\"-\".join([strPrefix, \"SAGEMAKER-ROLE-ARN\"]))\n",
    "strS3DataPath = pm.get_params(key=\"-\".join([strPrefix, \"S3-DATA-PATH\"]))\n",
    "tracking_server_arn = pm.get_params(key=\"-\".join([strPrefix, \"MLFLOW-TRACKING-SERVER-ARN\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc649e2-9aa5-4e99-9ea4-8cc80af63404",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix: ad-ts\n",
      "account_id: 419974056037\n",
      "defaulut_bucket: sm-anomaly-detection-dongjin\n",
      "sagemaker_role: arn:aws:iam::419974056037:role/sagemaker-mlflow\n",
      "s3_data_path: s3://sm-anomaly-detection-dongjin/data\n",
      "tracking_server_arn: arn:aws:sagemaker:us-east-1:419974056037:mlflow-tracking-server/mlflow-tracking-anomaly-detection\n"
     ]
    }
   ],
   "source": [
    "print (f\"prefix: {strPrefix}\")\n",
    "print (f\"account_id: {strAccountId}\")\n",
    "print (f\"defaulut_bucket: {strBucketName}\")\n",
    "print (f\"sagemaker_role: {strExecutionRole}\")\n",
    "print (f\"s3_data_path: {strS3DataPath}\")\n",
    "print (f\"tracking_server_arn: {tracking_server_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e7923d-c983-415d-ac13-34a51a56acb2",
   "metadata": {},
   "source": [
    "## 1. Data manipulation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ded2ca-ec05-4a08-94e9-5efb63472ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils.util import plot_click_w_fault_and_res, plot_click_w_fault_res_ad, plot_click_w_ad_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70cd3e-988e-40a8-87ed-59f65783c2e9",
   "metadata": {},
   "source": [
    "* load data and derive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f1088-9565-4429-93cb-80f4abb178f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clicks_1T = pd.read_csv(os.path.join(strS3DataPath, \"clicks_1T.csv\"), parse_dates=[\"timestamp\"]).set_index(\"timestamp\")\n",
    "clicks_1T[\"residual\"] = clicks_1T['click'] - clicks_1T['user'] \n",
    "clicks_1T[\"fault\"] = pd.read_csv(os.path.join(strS3DataPath, \"fault_label_1T.csv\"), header=None).values[0] ## label\n",
    "clicks_1T[\"time\"] = [int(str(time).split(\" \")[1].split(\":\")[0]) for time in clicks_1T.index] ## time variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cec5b1-86ac-4aed-87fc-994f097debe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f'data shape: {clicks_1T.shape}')\n",
    "print (f'timestamp min: {clicks_1T.index.min()}, max: {clicks_1T.index.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcee218-c0ea-4967-b50c-42f573c387c5",
   "metadata": {},
   "source": [
    "* visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc4bffc-5101-4668-8b4b-a157c6ada8e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_click_w_fault_and_res(clicks_1T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46e993-5015-42cd-b94b-9e7e4dcd4375",
   "metadata": {},
   "source": [
    "* upload data to s3 and local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c3df2f-9bbb-47cb-9312-5285a8e140c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strTrainDataName = \"merged_clicks_1T.csv\"\n",
    "clicks_1T.to_csv(os.path.join(strS3DataPath, strTrainDataName), index=True) # to s3\n",
    "clicks_1T.to_csv(os.path.join(\"./data\", strTrainDataName), index=True) # to local\n",
    "\n",
    "print (f'train_data_name: {strTrainDataName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d796a-cebe-4283-9364-f05ae7dd744c",
   "metadata": {},
   "source": [
    "## 2. Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba096c0-9ef1-4026-a2c2-a121bedb4708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import mlflow\n",
    "import argparse\n",
    "from os import path\n",
    "from pprint import pprint\n",
    "from pipeline_config.config import config_handler\n",
    "\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.steps import CacheConfig, ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession, LocalPipelineSession\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, FrameworkProcessor\n",
    "from sagemaker.workflow.retry import StepRetryPolicy, StepExceptionTypeEnum, SageMakerJobExceptionTypeEnum, SageMakerJobStepRetryPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32590397-ada9-4787-b58d-30759761d447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class mlops_pipeline():\n",
    "\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.strRegionName = self.args.config.get_value(\"COMMON\", \"region\")\n",
    "        self.pm = parameter_store(self.strRegionName)\n",
    "        self._env_setting()        \n",
    "\n",
    "    def _env_setting(self, ):\n",
    "\n",
    "        self.strPrefix = self.args.config.get_value(\"COMMON\", \"prefix\")\n",
    "        self.strExecutionRole = self.args.config.get_value(\"COMMON\", \"role\")\n",
    "        self.strBucketName = self.args.config.get_value(\"COMMON\", \"bucket\")\n",
    "        self.strModelName = self.args.config.get_value(\"COMMON\", \"model_name\")\n",
    "        self.strImageUri = self.args.config.get_value(\"COMMON\", \"image_uri\")\n",
    "        self.strPrepImageUri = self.args.config.get_value(\"COMMON\", \"image_uri_prep\")\n",
    "        self.strPipelineName = \"-\".join([self.strPrefix, self.strModelName])\n",
    "\n",
    "        self.cache_config = CacheConfig(\n",
    "            enable_caching=self.args.config.get_value(\"PIPELINE\", \"enable_caching\", dtype=\"boolean\"),\n",
    "            expire_after=self.args.config.get_value(\"PIPELINE\", \"expire_after\")\n",
    "        )\n",
    "\n",
    "        self.retry_policies=[\n",
    "            # retry when resource limit quota gets exceeded\n",
    "            SageMakerJobStepRetryPolicy(\n",
    "                exception_types=[SageMakerJobExceptionTypeEnum.RESOURCE_LIMIT],\n",
    "                expire_after_mins=180,\n",
    "                interval_seconds=60,\n",
    "                backoff_rate=1.0\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        if self.args.config.get_value(\"LOCALMODE\", \"mode\", dtype=\"boolean\"): self.pipeline_session = LocalPipelineSession()\n",
    "        else: self.pipeline_session = PipelineSession()\n",
    "\n",
    "        self.pm.put_params(key=\"-\".join([self.strPrefix, \"PIPELINE-NAME\"]), value=self.strPipelineName, overwrite=True)\n",
    "\n",
    "        print (f\" == Envrionment parameters == \")\n",
    "        print (f\"   SAGEMAKER-ROLE-ARN: {self.strExecutionRole}\")\n",
    "        print (f\"   PREFIX: {self.strPrefix}\")\n",
    "        print (f\"   BUCKET: {self.strBucketName}\")\n",
    "        print (f\"   IMAGE-URI: {self.strImageUri}\")\n",
    "\n",
    "    def _step_preprocessing(self, ):\n",
    "        \n",
    "        if self.args.config.get_value(\"LOCALMODE\", \"mode\", dtype=\"boolean\"):\n",
    "            pipeline_session = LocalPipelineSession()\n",
    "            instance_type = \"local\"\n",
    "        else:\n",
    "            pipeline_session = PipelineSession()\n",
    "            instance_type = self.args.config.get_value(\"PREPROCESSING\", \"instance_type\")\n",
    "            \n",
    "        strPrefixPrep = \"/opt/ml/processing/\"\n",
    "        strDataPath = self.args.config.get_value(\"PREPROCESSING\", \"data_path\")\n",
    "        strTrainDataName = self.args.config.get_value(\"PREPROCESSING\", \"data_name\")\n",
    "        \n",
    "        # network_config로 받으면 된다\n",
    "        prep_processor = FrameworkProcessor(\n",
    "            estimator_cls=PyTorch,\n",
    "            framework_version=self.args.config.get_value(\"PREPROCESSING\", \"framework_version\"),\n",
    "            py_version=\"py310\",\n",
    "            image_uri=None,\n",
    "            instance_type=instance_type,\n",
    "            instance_count=self.args.config.get_value(\"PREPROCESSING\", \"instance_count\", dtype=\"int\"),\n",
    "            role=self.strExecutionRole,\n",
    "            base_job_name=\"preprocessing\", # bucket에 보이는 이름 (pipeline으로 묶으면 pipeline에서 정의한 이름으로 bucket에 보임)\n",
    "            sagemaker_session=pipeline_session,\n",
    "            env={\"MLFLOW_TRACKING_ARN\": tracking_server_arn}\n",
    "        )\n",
    "        \n",
    "        step_args = prep_processor.run(\n",
    "            #job_name=\"preprocessing\", ## 이걸 넣어야 캐시가 작동함, 안그러면 프로세서의 base_job_name 이름뒤에 날짜 시간이 붙어서 캐시 동작 안함\n",
    "            #git_config=git_config,\n",
    "            code='preprocessing.py', #소스 디렉토리 안에서 파일 path\n",
    "            source_dir= \"./src/preprocessing\", #현재 파일에서 소스 디렉토리 상대경로 # add processing.py and requirements.txt here\n",
    "            inputs=[\n",
    "                ProcessingInput(\n",
    "                    input_name=\"input-data\",\n",
    "                    source=strDataPath,\n",
    "                    destination=os.path.join(strPrefixPrep, \"input\")\n",
    "                ),\n",
    "            ],\n",
    "            outputs=[\n",
    "                ProcessingOutput(\n",
    "                    output_name=\"output-data\",\n",
    "                    source=os.path.join(strPrefixPrep, \"output\"),\n",
    "                    destination=os.path.join(\n",
    "                        \"s3://{}\".format(self.strBucketName),\n",
    "                        self.strPipelineName,\n",
    "                        \"preprocessing\",\n",
    "                        \"output\"\n",
    "                    )\n",
    "                ),\n",
    "            ],\n",
    "            arguments=[\n",
    "                \"--proc_prefix\", strPrefixPrep, \\\n",
    "                \"--shingle_size\", str(self.args.config.get_value(\"PREPROCESSING\", \"shingle_size\", dtype=\"int\")), \\\n",
    "                \"--train_data_name\", strTrainDataName\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.preprocessing_process = ProcessingStep(\n",
    "            name=\"PreprocessingProcess\", ## Processing job이름\n",
    "            step_args=step_args,\n",
    "            cache_config=self.cache_config,\n",
    "        )\n",
    "        \n",
    "        print (\"  \\n== Preprocessing Step ==\")\n",
    "        print (\"   \\nArgs: \")\n",
    "\n",
    "        \n",
    "        for key, value in self.preprocessing_process.arguments.items():\n",
    "            print (\"===========================\")\n",
    "            print (f'key: {key}')\n",
    "            pprint (value)\n",
    "            \n",
    "        print (type(self.preprocessing_process.properties))\n",
    "            \n",
    "\n",
    "    def _step_training(self, ):\n",
    "        \n",
    "        if self.args.config.get_value(\"LOCALMODE\", \"mode\", dtype=\"boolean\"):\n",
    "            pipeline_session = LocalPipelineSession()\n",
    "            pipeline_session.config = {'local': {'local_code': True}}\n",
    "            instance_type = \"local_gpu\"\n",
    "        else:\n",
    "            pipeline_session = PipelineSession()\n",
    "            instance_type = self.args.config.get_value(\"TRAINING\", \"instance_type\")\n",
    "\n",
    "        dicHyperParams = {\n",
    "            \"epochs\":\"150\",\n",
    "            \"batch_size\":\"128\", \n",
    "            \"lr\":\"1e-2\",\n",
    "            \"shingle_size\":str(self.args.config.get_value(\"PREPROCESSING\", \"shingle_size\", dtype=\"int\")),\n",
    "            \"num_features\":\"4\",\n",
    "            \"emb_size\":\"4\",\n",
    "            \"workers\":\"2\"\n",
    "        }\n",
    "\n",
    "        strOutputPath = os.path.join(\n",
    "            \"s3://{}\".format(self.strBucketName),\n",
    "            self.strPipelineName,\n",
    "            \"training\",\n",
    "            \"model-output\"\n",
    "        )\n",
    "\n",
    "        strCodeLocation = os.path.join(\n",
    "            \"s3://{}\".format(self.strBucketName),\n",
    "            self.strPipelineName,\n",
    "            \"training\",\n",
    "            \"backup_codes\"\n",
    "        )\n",
    "\n",
    "        num_re = \"([0-9\\\\.]+)(e-?[[01][0-9])?\"\n",
    "        metric_definitions = [\n",
    "            {\"Name\": \"Train loss\", \"Regex\": f\"loss={num_re}\"},\n",
    "            {\"Name\": \"Train cos\", \"Regex\": f\"wer:{num_re}\"},\n",
    "            {\"Name\": \"Val cos\", \"Regex\": f\"wer:{num_re}\"}\n",
    "        ]\n",
    "\n",
    "        bSpotTraining = False\n",
    "        if bSpotTraining:\n",
    "            nMaxWait = 1*60*60\n",
    "            nMaxRun = 1*60*60\n",
    "\n",
    "        else:\n",
    "            nMaxWait = None\n",
    "            nMaxRun = 1*60*60\n",
    "\n",
    "        bUseTrainWarmPool = True ## training image 다운받지 않음, 속도 빨라진다\n",
    "        if bUseTrainWarmPool: nKeepAliveSeconds = 3600 ## 최대 1시간 동안!!, service quota에서 warmpool을 위한 request 필요\n",
    "        else: nKeepAliveSeconds = None\n",
    "        if bSpotTraining:\n",
    "            bUseTrainWarmPool = False # warmpool은 spot instance 사용시 활용 할 수 없음\n",
    "            nKeepAliveSeconds = None\n",
    "        \n",
    "        self.estimator = PyTorch(\n",
    "            entry_point=\"main.py\", # the script we want to run\n",
    "            source_dir=\"./src/training\", # where our conf/script is\n",
    "            #git_config=git_config,\n",
    "            role=self.strExecutionRole,\n",
    "            instance_type=instance_type,\n",
    "            instance_count=self.args.config.get_value(\"TRAINING\", \"instance_count\", dtype=\"int\"),\n",
    "            image_uri=None,\n",
    "            framework_version=self.args.config.get_value(\"TRAINING\", \"framework_version\"),\n",
    "            py_version=\"py310\",\n",
    "            volume_size=128,\n",
    "            code_location=strCodeLocation,\n",
    "            output_path=strOutputPath,\n",
    "            disable_profiler=True,\n",
    "            debugger_hook_config=False,\n",
    "            hyperparameters=dicHyperParams, #{'config-path': 'conf'},\n",
    "            #distribution={\"smdistributed\":{\"dataparallel\":{\"enabled\":True, \"fp16\": True}}},\n",
    "            sagemaker_session=pipeline_session,\n",
    "            metric_definitions=metric_definitions,\n",
    "            max_run=nMaxRun,\n",
    "            use_spot_instances=bSpotTraining,  # spot instance 활용\n",
    "            max_wait=nMaxWait,\n",
    "            keep_alive_period_in_seconds=nKeepAliveSeconds,\n",
    "            enable_sagemaker_metrics=True\n",
    "        )\n",
    "        \n",
    "        step_training_args = self.estimator.fit(\n",
    "            job_name=\"training\",\n",
    "            inputs={\n",
    "                \"train\": self.preprocessing_process.properties.ProcessingOutputConfig.Outputs[0].S3Output.S3Uri,\n",
    "                \"validation\": self.preprocessing_process.properties.ProcessingOutputConfig.Outputs[0].S3Output.S3Uri,\n",
    "            },\n",
    "            logs=\"All\",\n",
    "        )\n",
    "        \n",
    "        self.training_process = TrainingStep(\n",
    "            name=\"TrainingProcess\",\n",
    "            step_args=step_training_args,\n",
    "            cache_config=self.cache_config,\n",
    "            #depends_on=[self.preprocessing_process],\n",
    "            retry_policies=self.retry_policies\n",
    "        )\n",
    "\n",
    "        print (\"  \\n== Training Step ==\")\n",
    "        print (\"   \\nArgs: \")\n",
    "\n",
    "        for key, value in self.training_process.arguments.items():\n",
    "            print (\"===========================\")\n",
    "            print (f'key: {key}')\n",
    "            pprint (value)\n",
    "            \n",
    "    def _step_model_registration(self, ):\n",
    "        \n",
    "        self.strModelPackageGroupName = \"-\".join([\"MPG\", self.strPrefix, self.strModelName])\n",
    "        self.pm.put_params(key=\"-\".join([self.strPrefix, \"MODEL-GROUP-NAME\"]), value=self.strModelPackageGroupName, overwrite=True)\n",
    "                                                                              \n",
    "        # model_metrics = ModelMetrics(\n",
    "        #     model_statistics=MetricsSource(\n",
    "        #         s3_uri=Join(\n",
    "        #             on=\"/\",\n",
    "        #             values=[\n",
    "        #                 self.evaluation_process.properties.ProcessingOutputConfig.Outputs[\"evaluation-metrics\"].S3Output.S3Uri,\n",
    "        #                 #print (self.evaluation_process.arguments.items())로 확인가능\n",
    "        #                 f\"evaluation-{self.strModelName}.json\"\n",
    "        #             ],\n",
    "        #         ),\n",
    "        #         content_type=\"application/json\")\n",
    "        # )\n",
    "        \n",
    "        model = PyTorchModel(\n",
    "            source_dir=\"./src/deploy\",\n",
    "            entry_point=\"inference.py\",\n",
    "            model_data=self.training_process.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            role=self.strExecutionRole,\n",
    "            framework_version=self.args.config.get_value(\"MODEL_REGISTER\", \"framework_version\"),\n",
    "            py_version='py310',\n",
    "            model_server_workers=1,\n",
    "            code_location=os.path.join(\n",
    "                \"s3://\",\n",
    "                self.strBucketName,\n",
    "                self.strPipelineName,\n",
    "                \"inference\",\n",
    "                \"model\"\n",
    "            ),\n",
    "            sagemaker_session=self.pipeline_session,\n",
    "        )\n",
    "\n",
    "        step_args = model.register(\n",
    "            content_types=[\"application/json\", \"file-path/raw-bytes\", \"text/csv\"],\n",
    "            response_types=[\"application/json\"],\n",
    "            inference_instances=self.args.config.get_value(\"MODEL_REGISTER\", \"inference_instances\", dtype=\"list\"),\n",
    "            transform_instances=self.args.config.get_value(\"MODEL_REGISTER\", \"transform_instances\", dtype=\"list\"),\n",
    "            model_package_group_name=self.strModelPackageGroupName,\n",
    "            approval_status=self.args.config.get_value(\"MODEL_REGISTER\", \"model_approval_status_default\"),\n",
    "            ## “Approved”, “Rejected”, or “PendingManualApproval” (default: “PendingManualApproval”).\n",
    "            #model_metrics=model_metrics\n",
    "        )\n",
    "\n",
    "        self.register_process = ModelStep(\n",
    "            name=\"ModelRegisterProcess\",\n",
    "            step_args=step_args,\n",
    "            #depends_on=[self.evaluation_process]\n",
    "        )\n",
    "        \n",
    "    def _step_deploy(self, ):\n",
    "        \n",
    "        strInstanceType = self.args.config.get_value(\"DEPLOY\", \"processing_instance_type\")\n",
    "        nInstanceCount = self.args.config.get_value(\"DEPLOY\", \"processing_instance_count\", dtype=\"int\")\n",
    "        strDepolyInstanceType = self.args.config.get_value(\"DEPLOY\", \"instance_type\")\n",
    "        strEndpointName = f'endpoint--{self.strPipelineName}-{int(time.time())}'\n",
    "        strProcPrefixPath = \"/opt/ml/processing\"\n",
    "        \n",
    "        deploy_processor = FrameworkProcessor(\n",
    "            estimator_cls=PyTorch,\n",
    "            framework_version=self.args.config.get_value(\"DEPLOY\", \"processing_framework_version\"),\n",
    "            py_version=\"py310\",\n",
    "            image_uri=None,\n",
    "            role=self.strExecutionRole,\n",
    "            instance_type=strInstanceType,\n",
    "            instance_count=nInstanceCount,\n",
    "            base_job_name=\"deploy\", # bucket에 보이는 이름 (pipeline으로 묶으면 pipeline에서 정의한 이름으로 bucket에 보임)\n",
    "            sagemaker_session=self.pipeline_session\n",
    "        )\n",
    "        \n",
    "        step_deploy_args = deploy_processor.run(\n",
    "            code=\"deploy.py\",\n",
    "            source_dir=\"src/deploy/\",\n",
    "            arguments=[\n",
    "                \"--prefix_deploy\", strProcPrefixPath, \\\n",
    "                \"--region\", self.strRegionName, \\\n",
    "                \"--instance_type\", strInstanceType, \\\n",
    "                \"--depoly_instance_type\", strDepolyInstanceType, \\\n",
    "                \"--model_package_group_name\", self.strModelPackageGroupName, \\\n",
    "                \"--endpoint_name\", strEndpointName, \\\n",
    "                \"--execution_role\", self.strExecutionRole, \\\n",
    "            ],\n",
    "            job_name=\"deploy\",\n",
    "        )\n",
    "        \n",
    "        self.pm.put_params(key=self.strPrefix + \"-ENDPOINT-NAME\", value=strEndpointName, overwrite=True)\n",
    "        \n",
    "        self.deploy_process = ProcessingStep(\n",
    "            name=\"DeployProcess\", ## Processing job이름\n",
    "            step_args=step_deploy_args,\n",
    "            depends_on=[self.register_process],\n",
    "            cache_config=self.cache_config,\n",
    "            retry_policies=self.retry_policies\n",
    "        )\n",
    "        \n",
    "        print (\"  \\n== Deploy Step ==\")\n",
    "        print (\"   \\nArgs: \")\n",
    "\n",
    "        for key, value in self.deploy_process.arguments.items():\n",
    "            print (\"===========================\")\n",
    "            print (f'key: {key}')\n",
    "            pprint (value)\n",
    "            \n",
    "    def _get_pipeline(self, ):\n",
    "\n",
    "        pipeline = Pipeline(\n",
    "            name=self.strPipelineName,\n",
    "            #steps=[self.preprocessing_process, self.training_process, self.register_process, self.deploy_process],\n",
    "            steps=[self.preprocessing_process],\n",
    "            sagemaker_session=self.pipeline_session\n",
    "        )\n",
    "\n",
    "        return pipeline\n",
    "\n",
    "    def execution(self, ):\n",
    "\n",
    "        self._step_preprocessing()\n",
    "        #self._step_training()\n",
    "        #self._step_model_registration()\n",
    "        #self._step_deploy()\n",
    "\n",
    "        pipeline = self._get_pipeline()\n",
    "        pipeline.upsert(role_arn=self.strExecutionRole) ## Submit the pipeline definition to the SageMaker Pipelines service \n",
    "        execution = pipeline.start()\n",
    "        desc = execution.describe()\n",
    "        \n",
    "        self.pm.put_params(\n",
    "            key=\"-\".join([self.strPrefix, \"PIPELINE-ARN\"]),\n",
    "            value=desc[\"PipelineArn\"],\n",
    "            overwrite=True\n",
    "        )\n",
    "        print (\"PipelineArn: \", desc[\"PipelineArn\"])\n",
    "        print (execution.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088b8f8-f818-4b5d-8bba-34f86709afac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# strBasePath, strCurrentDir = path.dirname(path.abspath(__file__)), os.getcwd()\n",
    "# os.chdir(strBasePath)\n",
    "# print (\"==================\")\n",
    "# print (f\"  Working Dir: {os.getcwd()}\")\n",
    "# print (f\"  You should execute 'mlops_pipeline.py' in 'pipeline' directory'\") \n",
    "# print (\"==================\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args, _ = parser.parse_known_args()\n",
    "args.config = config_handler()\n",
    "\n",
    "print(\"Received arguments {}\".format(args))\n",
    "os.environ['AWS_DEFAULT_REGION'] = args.config.get_value(\"COMMON\", \"region\")\n",
    "\n",
    "pipe_tr = mlops_pipeline(args)\n",
    "pipe_tr.execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d6257d-535f-494d-bfdf-31ef20cf8f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421bd50-9651-4fed-8ce0-73f81bad07b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
