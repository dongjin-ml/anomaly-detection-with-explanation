{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43eaad7f-f315-4f50-938a-fd94f07ca15a",
   "metadata": {},
   "source": [
    "# <B> SageMaker pileline for Anormaly Detection based on AutoEncoder </B>\n",
    "* Container: codna_pytorch_p310"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e8867-4692-4d7c-8009-127c7125489b",
   "metadata": {},
   "source": [
    "## AutoReload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b26cf7-b510-4508-b86f-bd18e3e96a17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013479d-b181-48ed-8cd4-bfa68a82131f",
   "metadata": {},
   "source": [
    "## parameter store 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d5bbe6-5c31-40e0-be7b-0de1935e7fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from utils.ssm import parameter_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ffc18b-f73c-4bde-9436-93c378ed3175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strRegionName=boto3.Session().region_name\n",
    "pm = parameter_store(strRegionName)\n",
    "strPrefix = pm.get_params(key=\"PREFIX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842031c-56e5-4728-8411-669ae40bac99",
   "metadata": {},
   "source": [
    "## pramamters for tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee18fe5f-e302-4913-9d7e-534aae3ad98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strAccountId = pm.get_params(key=\"-\".join([strPrefix, \"ACCOUNT-ID\"]))\n",
    "strBucketName = pm.get_params(key=\"-\".join([strPrefix, \"BUCKET\"]))\n",
    "strExecutionRole = pm.get_params(key=\"-\".join([strPrefix, \"SAGEMAKER-ROLE-ARN\"]))\n",
    "strS3DataPath = pm.get_params(key=\"-\".join([strPrefix, \"S3-DATA-PATH\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc649e2-9aa5-4e99-9ea4-8cc80af63404",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix: ad-ts\n",
      "account_id: 419974056037\n",
      "defaulut_bucket: sm-anomaly-detection-dongjin\n",
      "sagemaker_role: arn:aws:iam::419974056037:role/service-role/AmazonSageMaker-ExecutionRole-20221206T163436\n",
      "s3_data_path: s3://sm-anomaly-detection-dongjin/data\n"
     ]
    }
   ],
   "source": [
    "print (f\"prefix: {strPrefix}\")\n",
    "print (f\"account_id: {strAccountId}\")\n",
    "print (f\"defaulut_bucket: {strBucketName}\")\n",
    "print (f\"sagemaker_role: {strExecutionRole}\")\n",
    "print (f\"s3_data_path: {strS3DataPath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e7923d-c983-415d-ac13-34a51a56acb2",
   "metadata": {},
   "source": [
    "## 1. Data manipulation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ded2ca-ec05-4a08-94e9-5efb63472ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils.util import plot_click_w_fault_and_res, plot_click_w_fault_res_ad, plot_click_w_ad_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70cd3e-988e-40a8-87ed-59f65783c2e9",
   "metadata": {},
   "source": [
    "* load data and derive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f1088-9565-4429-93cb-80f4abb178f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clicks_1T = pd.read_csv(os.path.join(strS3DataPath, \"clicks_1T.csv\"), parse_dates=[\"timestamp\"]).set_index(\"timestamp\")\n",
    "clicks_1T[\"residual\"] = clicks_1T['click'] - clicks_1T['user'] \n",
    "clicks_1T[\"fault\"] = pd.read_csv(os.path.join(strS3DataPath, \"fault_label_1T.csv\"), header=None).values[0] ## label\n",
    "clicks_1T[\"time\"] = [int(str(time).split(\" \")[1].split(\":\")[0]) for time in clicks_1T.index] ## time variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cec5b1-86ac-4aed-87fc-994f097debe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f'data shape: {clicks_1T.shape}')\n",
    "print (f'timestamp min: {clicks_1T.index.min()}, max: {clicks_1T.index.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcee218-c0ea-4967-b50c-42f573c387c5",
   "metadata": {},
   "source": [
    "* visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc4bffc-5101-4668-8b4b-a157c6ada8e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_click_w_fault_and_res(clicks_1T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46e993-5015-42cd-b94b-9e7e4dcd4375",
   "metadata": {},
   "source": [
    "* upload data to s3 and local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c3df2f-9bbb-47cb-9312-5285a8e140c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strTrainDataName = \"merged_clicks_1T.csv\"\n",
    "clicks_1T.to_csv(os.path.join(strS3DataPath, strTrainDataName), index=True) # to s3\n",
    "clicks_1T.to_csv(os.path.join(\"./data\", strTrainDataName), index=True) # to local\n",
    "\n",
    "print (f'train_data_name: {strTrainDataName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d796a-cebe-4283-9364-f05ae7dd744c",
   "metadata": {},
   "source": [
    "## 2. Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba096c0-9ef1-4026-a2c2-a121bedb4708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from os import path\n",
    "from pprint import pprint\n",
    "from pipeline_config.config import config_handler\n",
    "\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CacheConfig, ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession, LocalPipelineSession\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, FrameworkProcessor\n",
    "from sagemaker.workflow.retry import StepRetryPolicy, StepExceptionTypeEnum, SageMakerJobExceptionTypeEnum, SageMakerJobStepRetryPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32590397-ada9-4787-b58d-30759761d447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class mlops_pipeline():\n",
    "\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.strRegionName = self.args.config.get_value(\"COMMON\", \"region\")\n",
    "        self.pm = parameter_store(self.strRegionName)\n",
    "        self._env_setting()        \n",
    "\n",
    "    def _env_setting(self, ):\n",
    "        \n",
    "        self.strPrefix = self.args.config.get_value(\"COMMON\", \"prefix\")\n",
    "        self.strExcutionRole = self.args.config.get_value(\"COMMON\", \"role\")\n",
    "        self.strBucketName = self.args.config.get_value(\"COMMON\", \"bucket\")\n",
    "        self.strModelName = self.args.config.get_value(\"COMMON\", \"model_name\")\n",
    "        self.strImageUri = self.args.config.get_value(\"COMMON\", \"image_uri\")\n",
    "        self.strPrepImageUri = self.args.config.get_value(\"COMMON\", \"image_uri_prep\")\n",
    "        self.strPipelineName = \"-\".join([self.strPrefix, self.strModelName])\n",
    "            \n",
    "        self.cache_config = CacheConfig(\n",
    "            enable_caching=self.args.config.get_value(\"PIPELINE\", \"enable_caching\", dtype=\"boolean\"),\n",
    "            expire_after=self.args.config.get_value(\"PIPELINE\", \"expire_after\")\n",
    "        )\n",
    "        \n",
    "        self.retry_policies=[                \n",
    "            # retry when resource limit quota gets exceeded\n",
    "            SageMakerJobStepRetryPolicy(\n",
    "                exception_types=[SageMakerJobExceptionTypeEnum.RESOURCE_LIMIT],\n",
    "                expire_after_mins=180,\n",
    "                interval_seconds=60,\n",
    "                backoff_rate=1.0\n",
    "            ),\n",
    "        ]\n",
    "        \n",
    "        # self.git_config = {\n",
    "        #     'repo': f'https://{self.pm.get_params(key=\"-\".join([self.strPrefix, \"CODE-REPO\"]))}',\n",
    "        #     'branch': 'main',\n",
    "        #     'username': self.pm.get_params(key=\"-\".join([self.strPrefix, \"CODECOMMIT-USERNAME\"]), enc=True),\n",
    "        #     'password': self.pm.get_params(key=\"-\".join([self.strPrefix, \"CODECOMMIT-PWD\"]), enc=True)\n",
    "        # }\n",
    "        \n",
    "        if self.args.config.get_value(\"LOCALMODE\", \"mode\", dtype=\"boolean\"): self.pipeline_session = LocalPipelineSession()\n",
    "        else: self.pipeline_session = PipelineSession()\n",
    "        \n",
    "        self.pm.put_params(key=\"-\".join([self.strPrefix, \"PIPELINE-NAME\"]), value=self.strPipelineName, overwrite=True)\n",
    "        \n",
    "        print (f\" == Envrionment parameters == \")\n",
    "        print (f\"   SAGEMAKER-ROLE-ARN: {self.strExcutionRole}\")\n",
    "        print (f\"   PREFIX: {self.strPrefix}\")\n",
    "        print (f\"   BUCKET: {self.strBucketName}\")\n",
    "        print (f\"   IMAGE-URI: {self.strImageUri}\")\n",
    "        \n",
    "    def _step_preprocessing(self, ):\n",
    "        \n",
    "        if self.args.config.get_value(\"LOCALMODE\", \"mode\", dtype=\"boolean\"):\n",
    "            pipeline_session = LocalPipelineSession()\n",
    "        else:\n",
    "            pipeline_session = PipelineSession()\n",
    "            \n",
    "        strPrefixPrep = \"/opt/ml/processing/\"\n",
    "        strDataPath = self.args.config.get_value(\"PREPROCESSING\", \"data_path\")\n",
    "        strTrainDataName = self.args.config.get_value(\"PREPROCESSING\", \"data_name\")\n",
    "        \n",
    "        # network_config로 받으면 된다\n",
    "        prep_processor = FrameworkProcessor(\n",
    "            estimator_cls=PyTorch,\n",
    "            framework_version=self.args.config.get_value(\"PREPROCESSING\", \"framework_version\"),\n",
    "            py_version=\"py310\",\n",
    "            image_uri=None,\n",
    "            instance_type=self.args.config.get_value(\"PREPROCESSING\", \"instance_type\"),\n",
    "            instance_count=self.args.config.get_value(\"PREPROCESSING\", \"instance_count\", dtype=\"int\"),\n",
    "            role=self.strExcutionRole,\n",
    "            base_job_name=\"preprocessing\", # bucket에 보이는 이름 (pipeline으로 묶으면 pipeline에서 정의한 이름으로 bucket에 보임)\n",
    "            sagemaker_session=pipeline_session\n",
    "        )\n",
    "        \n",
    "        step_args = prep_processor.run(\n",
    "            #job_name=\"preprocessing\", ## 이걸 넣어야 캐시가 작동함, 안그러면 프로세서의 base_job_name 이름뒤에 날짜 시간이 붙어서 캐시 동작 안함\n",
    "            #git_config=git_config,\n",
    "            code='preprocessing.py', #소스 디렉토리 안에서 파일 path\n",
    "            source_dir= \"./src/preprocessing\", #현재 파일에서 소스 디렉토리 상대경로 # add processing.py and requirements.txt here\n",
    "            inputs=[\n",
    "                ProcessingInput(\n",
    "                    input_name=\"input-data\",\n",
    "                    source=strDataPath,\n",
    "                    destination=os.path.join(strPrefixPrep, \"input\")\n",
    "                ),\n",
    "            ],\n",
    "            outputs=[\n",
    "                ProcessingOutput(\n",
    "                    output_name=\"output-data\",\n",
    "                    source=os.path.join(strPrefixPrep, \"output\"),\n",
    "                    destination=os.path.join(\n",
    "                        \"s3://{}\".format(self.strBucketName),\n",
    "                        self.strPipelineName,\n",
    "                        \"preprocessing\",\n",
    "                        \"output\"\n",
    "                    )\n",
    "                ),\n",
    "            ],\n",
    "            arguments=[\n",
    "                \"--proc_prefix\", strPrefixPrep, \\\n",
    "                \"--shingle_size\", str(self.args.config.get_value(\"PREPROCESSING\", \"shingle_size\", dtype=\"int\")), \\\n",
    "                \"--train_data_name\", strTrainDataName\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        # prep_processor = FrameworkProcessor(\n",
    "        #     estimator_cls=SKLearn,\n",
    "        #     image_uri=self.strPrepImageUri,\n",
    "        #     framework_version=self.args.config.get_value(\"PREPROCESSING\", \"framework_version\"),\n",
    "        #     role=self.strExcutionRole,\n",
    "        #     instance_type=self.args.config.get_value(\"PREPROCESSING\", \"instance_type\"),\n",
    "        #     instance_count=self.args.config.get_value(\"PREPROCESSING\", \"instance_count\", dtype=\"int\"),\n",
    "        #     base_job_name=\"preprocessing\", # bucket에 보이는 이름 (pipeline으로 묶으면 pipeline에서 정의한 이름으로 bucket에 보임)\n",
    "        #     sagemaker_session=pipeline_session\n",
    "        # )\n",
    "            \n",
    "        # step_args = prep_processor.run(\n",
    "        #     code='./preprocessing.py', #소스 디렉토리 안에서 파일 path\n",
    "        #     source_dir=\"./sources/preprocessing/\", #현재 파일에서 소스 디렉토리 상대경로 # add processing.py and requirements.txt here\n",
    "        #     git_config=self.git_config,\n",
    "        #     inputs=[\n",
    "        #         ProcessingInput(\n",
    "        #             input_name=\"input\",\n",
    "        #             source=strDataPath,\n",
    "        #             destination=os.path.join(strPrefixPrep, \"input\")\n",
    "        #         ),\n",
    "        #     ],\n",
    "        #     outputs=[\n",
    "        #         ProcessingOutput(\n",
    "        #             output_name=\"train-data\",\n",
    "        #             source=os.path.join(strPrefixPrep, \"output\", \"train\"),\n",
    "        #             destination=os.path.join(\n",
    "        #                 \"s3://{}\".format(self.strBucketName),\n",
    "        #                 self.strPipelineName,\n",
    "        #                 \"preprocessing\",\n",
    "        #                 \"output\",\n",
    "        #                 \"train-data\"\n",
    "        #             ),\n",
    "        #         ),\n",
    "        #         ProcessingOutput(\n",
    "        #             output_name=\"validation-data\",\n",
    "        #             source=os.path.join(strPrefixPrep, \"output\", \"validation\"),\n",
    "        #             destination=os.path.join(\n",
    "        #                 \"s3://{}\".format(self.strBucketName),\n",
    "        #                 self.strPipelineName,\n",
    "        #                 \"preprocessing\",\n",
    "        #                 \"output\",\n",
    "        #                 \"validation-data\",\n",
    "        #             ),\n",
    "        #         ),\n",
    "        #         ProcessingOutput(\n",
    "        #             output_name=\"test-data\",\n",
    "        #             source=os.path.join(strPrefixPrep, \"output\", \"test\"),\n",
    "        #             destination=os.path.join(\n",
    "        #                 \"s3://{}\".format(self.strBucketName),\n",
    "        #                 self.strPipelineName,\n",
    "        #                 \"preprocessing\",\n",
    "        #                 \"output\",\n",
    "        #                 \"test-data\",\n",
    "        #             ),\n",
    "        #         )\n",
    "        #     ],\n",
    "        #     arguments=[\"--prefix_prep\", strPrefixPrep, \"--region\", self.strRegionName],\n",
    "        #     job_name=\"preprocessing\",\n",
    "        # )\n",
    "        \n",
    "        self.preprocessing_process = ProcessingStep(\n",
    "            name=\"PreprocessingProcess\", ## Processing job이름\n",
    "            step_args=step_args,\n",
    "            cache_config=self.cache_config,\n",
    "        )\n",
    "        \n",
    "        print (\"  \\n== Preprocessing Step ==\")\n",
    "        print (\"   \\nArgs: \")\n",
    "        for key, value in self.preprocessing_process.arguments.items():\n",
    "            print (\"===========================\")\n",
    "            print (f'key: {key}')\n",
    "            pprint (value)\n",
    "            \n",
    "        print (type(self.preprocessing_process.properties))\n",
    "            \n",
    "\n",
    "#     def _step_training(self, ):\n",
    "        \n",
    "#         if self.args.config.get_value(\"LOCALMODE\", \"mode\", dtype=\"boolean\"):\n",
    "#             pipeline_session = LocalPipelineSession()\n",
    "#             pipeline_session.config = {'local': {'local_code': True}}\n",
    "#         else:\n",
    "#             pipeline_session = PipelineSession()\n",
    "                \n",
    "#         dicHyperparameters = {  \n",
    "#             \"max_depth\": \"10\",\n",
    "#             \"eta\": \"0.3\",\n",
    "#             \"objective\": \"reg:squarederror\",\n",
    "#             \"num_round\": \"100\",\n",
    "#         }\n",
    "        \n",
    "#         self.estimator = XGBoost(\n",
    "#             entry_point=\"xgboost_regression.py\",\n",
    "#             source_dir=\"./sources/train/\", #현재 파일에서 소스 디렉토리 상대경로 # add processing.py and requirements.txt here\n",
    "#             git_config=self.git_config,\n",
    "#             hyperparameters=dicHyperparameters, ## Contatiner내 env. variable로 들어 감\n",
    "#             role=self.strExcutionRole,\n",
    "#             instance_count=self.args.config.get_value(\"TRAINING\", \"instance_count\", dtype=\"int\"),\n",
    "#             instance_type=self.args.config.get_value(\"TRAINING\", \"instance_type\"),\n",
    "#             framework_version=self.args.config.get_value(\"TRAINING\", \"framework_version\"),\n",
    "#             image_uri = self.strImageUri,\n",
    "#             enable_sagemaker_metrics=True,\n",
    "#             volume_size=64, ## GB\n",
    "#             output_path=os.path.join(\n",
    "#                 \"s3://{}\".format(self.strBucketName),\n",
    "#                 self.strPipelineName,\n",
    "#                 \"training\",\n",
    "#                 \"output\"\n",
    "#             ),\n",
    "#             base_job_name=\"xgboost-train\",\n",
    "#             sagemaker_session=pipeline_session,\n",
    "#             #metric_definitions=listMetricDefinitions\n",
    "#         )\n",
    "        \n",
    "#         step_training_args = self.estimator.fit(\n",
    "#             job_name=\"training\",\n",
    "#             inputs={\n",
    "#                 \"TR\": self.preprocessing_process.properties.ProcessingOutputConfig.Outputs[\"train-data\"].S3Output.S3Uri,\n",
    "#                 \"VAL\": self.preprocessing_process.properties.ProcessingOutputConfig.Outputs[\"validation-data\"].S3Output.S3Uri,\n",
    "#                 \"TE\": self.preprocessing_process.properties.ProcessingOutputConfig.Outputs[\"test-data\"].S3Output.S3Uri,\n",
    "#             },\n",
    "#             logs=\"All\",\n",
    "#         )\n",
    "          \n",
    "#         self.training_process = TrainingStep(\n",
    "#             name=\"TrainingProcess\",\n",
    "#             step_args=step_training_args,\n",
    "#             cache_config=self.cache_config,\n",
    "#             #depends_on=[self.preprocessing_process],\n",
    "#             retry_policies=self.retry_policies\n",
    "#         )\n",
    "        \n",
    "#         print (\"  \\n== Training Step ==\")\n",
    "#         print (\"   \\nArgs: \")\n",
    "#         for key, value in self.training_process.arguments.items():\n",
    "#             print (\"===========================\")\n",
    "#             print (f'key: {key}')\n",
    "#             pprint (value)\n",
    "        \n",
    "#     def _step_evaluation(self, ):\n",
    "        \n",
    "#         #https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job-frameworks-pytorch.html\n",
    "        \n",
    "#         if self.args.config.get_value(\"LOCALMODE\", \"mode\", dtype=\"boolean\"):\n",
    "#             pipeline_session = LocalPipelineSession()\n",
    "#         else:\n",
    "#             pipeline_session = PipelineSession()\n",
    "            \n",
    "#         strPrefixPrep = \"/opt/ml/processing/\"\n",
    "        \n",
    "#         #Initialize the XGBoostProcessor\n",
    "#         eval_processor = XGBoostProcessor(\n",
    "#             image_uri=self.strImageUri, \n",
    "#             framework_version=self.args.config.get_value(\"EVALUATION\", \"framework_version\"),\n",
    "#             role=self.strExcutionRole,\n",
    "#             instance_type=self.args.config.get_value(\"EVALUATION\", \"instance_type\"),\n",
    "#             instance_count=self.args.config.get_value(\"EVALUATION\", \"instance_count\", dtype=\"int\"),\n",
    "#             base_job_name='evaluation',\n",
    "#             sagemaker_session=pipeline_session,\n",
    "#         )\n",
    "                \n",
    "#         self.evaluation_report = PropertyFile(\n",
    "#             name=\"EvaluationReport\",\n",
    "#             output_name=\"evaluation-metrics\",\n",
    "#             path=\"evaluation-\" + self.strModelName +  \".json\",\n",
    "#         )\n",
    "        \n",
    "#         step_args = eval_processor.run(\n",
    "#             job_name=\"evaluation\", # Evaluation job name. If not specified, the processor generates a default job name, based on the base job name and current timestamp.\n",
    "#                                    # 이걸 넣어야 캐시가 작동함, 안그러면 프로세서의 base_job_name 이름뒤에 날짜 시간이 붙어서 캐시 동작 안함\n",
    "#             code='evaluation.py', #소스 디렉토리 안에서 파일 path\n",
    "#             source_dir=\"./sources/evaluation/\", #현재 파일에서 소스 디렉토리 상대경로 # add processing.py and requirements.txt here\n",
    "#             git_config=self.git_config,\n",
    "            \n",
    "#             inputs=[\n",
    "#                 ProcessingInput(\n",
    "#                     source=self.training_process.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "#                     destination=os.path.join(strPrefixPrep, \"model\") #\"/opt/ml/processing/model\"\n",
    "#                 ),\n",
    "#                 ProcessingInput(\n",
    "#                     source=self.preprocessing_process.properties.ProcessingOutputConfig.Outputs[\"test-data\"].S3Output.S3Uri,\n",
    "#                     destination=os.path.join(strPrefixPrep, \"test\") #\"/opt/ml/processing/test\"\n",
    "#                 )\n",
    "#             ],\n",
    "#             outputs=[\n",
    "#                 ProcessingOutput(\n",
    "#                     output_name=\"evaluation-metrics\",\n",
    "#                     source=os.path.join(strPrefixPrep, \"evaluation\"), #\"/opt/ml/processing/evaluation\",\n",
    "#                     destination=os.path.join(\n",
    "#                         \"s3://{}\".format(self.strBucketName),\n",
    "#                         self.strPipelineName,\n",
    "#                         \"evaluation\",\n",
    "#                         \"evaluation-metrics\",\n",
    "#                     ),\n",
    "#                 )\n",
    "#             ],\n",
    "#             arguments=[\"--s3_model_path\", self.training_process.properties.ModelArtifacts.S3ModelArtifacts, \\\n",
    "#                        \"--region\", self.strRegionName, \"--model_name\", self.strModelName, \\\n",
    "#                        \"--prefix_eval\", strPrefixPrep]\n",
    "#         )\n",
    "        \n",
    "#         self.evaluation_process = ProcessingStep(\n",
    "#             name=\"EvaluationProcess\", ## Processing job이름들\n",
    "#             step_args=step_args,\n",
    "#             #depends_on=[self.preprocessing_process, self.training_process],\n",
    "#             property_files=[self.evaluation_report],\n",
    "#             cache_config=self.cache_config,\n",
    "#             retry_policies=self.retry_policies\n",
    "#         )\n",
    "        \n",
    "#         print (\"  \\n== Evaluation Step ==\")\n",
    "#         print (\"   \\nArgs: \")\n",
    "#         for key, value in self.evaluation_process.arguments.items():\n",
    "#             print (\"===========================\")\n",
    "#             print (f'key: {key}')\n",
    "#             pprint (value)\n",
    "        \n",
    "#     def _step_model_registration(self, ):\n",
    "        \n",
    "#         self.strModelPackageGroupName = \"-\".join([\"MPG\", self.strPrefix, self.strModelName])\n",
    "#         self.pm.put_params(key=\"-\".join([self.strPrefix, \"MODEL-GROUP-NAME\"]), value=self.strModelPackageGroupName, overwrite=True)\n",
    "                                                                              \n",
    "#         model_metrics = ModelMetrics(\n",
    "#             model_statistics=MetricsSource(\n",
    "#                 s3_uri=Join(\n",
    "#                     on=\"/\",\n",
    "#                     values=[\n",
    "#                         self.evaluation_process.properties.ProcessingOutputConfig.Outputs[\"evaluation-metrics\"].S3Output.S3Uri,\n",
    "#                         #print (self.evaluation_process.arguments.items())로 확인가능\n",
    "#                         f\"evaluation-{self.strModelName}.json\"\n",
    "#                     ],\n",
    "#                 ),\n",
    "#                 content_type=\"application/json\")\n",
    "#         )\n",
    "        \n",
    "#         model = XGBoostModel(\n",
    "#             entry_point=\"inference.py\",\n",
    "#             source_dir=\"./sources/inference/\",\n",
    "#             git_config=self.git_config,\n",
    "#             framework_version=self.args.config.get_value(\"MODEL_REGISTER\", \"framework_version\"),\n",
    "#             code_location=os.path.join(\n",
    "#                 \"s3://\",\n",
    "#                 self.strBucketName,\n",
    "#                 self.strPipelineName,\n",
    "#                 \"inference\",\n",
    "#                 \"model\"\n",
    "#             ),\n",
    "#             model_data=self.training_process.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "#             role=self.strExcutionRole,\n",
    "#             image_uri=self.strImageUri,\n",
    "#             sagemaker_session=self.pipeline_session,\n",
    "#         )\n",
    "        \n",
    "#         step_args = model.register(\n",
    "#             content_types=[\"file-path/raw-bytes\", \"text/csv\"],\n",
    "#             response_types=[\"application/json\"],\n",
    "#             inference_instances=self.args.config.get_value(\"MODEL_REGISTER\", \"inference_instances\", dtype=\"list\"),\n",
    "#             transform_instances=self.args.config.get_value(\"MODEL_REGISTER\", \"transform_instances\", dtype=\"list\"),\n",
    "#             model_package_group_name=self.strModelPackageGroupName,\n",
    "#             approval_status=self.args.config.get_value(\"MODEL_REGISTER\", \"model_approval_status_default\"),\n",
    "#             ## “Approved”, “Rejected”, or “PendingManualApproval” (default: “PendingManualApproval”).\n",
    "#             model_metrics=model_metrics,\n",
    "            \n",
    "#         )\n",
    "#         self.register_process = ModelStep(\n",
    "#             name=\"ModelRegisterProcess\",\n",
    "#             step_args=step_args,\n",
    "#             depends_on=[self.evaluation_process]\n",
    "#         )\n",
    "              \n",
    "#     def _step_fail(self, ):\n",
    "            \n",
    "#         self.fail_process = FailStep(\n",
    "#             name=\"ConditionFail\",\n",
    "#             error_message=Join(\n",
    "#                 on=\" \",\n",
    "#                 values=[\"Execution failed due to performance threshold\"]\n",
    "#             ),\n",
    "#         )\n",
    "        \n",
    "#     def _step_condition(self, ):\n",
    "        \n",
    "#         if self.args.config.get_value(\"LOCALMODE\", \"mode\", dtype=\"boolean\"):\n",
    "#             self.pipeline_session = LocalPipelineSession()\n",
    "#         else:\n",
    "#             self.pipeline_session = PipelineSession()\n",
    "        \n",
    "#         # https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/build-and-manage-steps.html#step-type-condition\n",
    "#         # 조건문 종류: https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_model_building_pipeline.html#conditions\n",
    "        \n",
    "#         self.condition_acc = ConditionGreaterThanOrEqualTo(\n",
    "#             left=JsonGet(\n",
    "#                 step_name=self.evaluation_process.name,\n",
    "#                 property_file=self.evaluation_report,\n",
    "#                 json_path=\"performance_metrics.mse.value\" ## evaluation.py에서 json으로 performance를 기록한 대로 한다. \n",
    "#                                                                ## 즉, S3에 저장된 evaluation-<model_name>.json 파일안에 있는 값을 적어줘야 한다. \n",
    "#             ),\n",
    "#             right=self.args.config.get_value(\"CONDITION\", \"thesh_mse\", dtype=\"float\"),\n",
    "#         )\n",
    "        \n",
    "#         self.condition_prec = ConditionGreaterThanOrEqualTo(\n",
    "#             left=JsonGet(\n",
    "#                 step_name=self.evaluation_process.name,\n",
    "#                 property_file=self.evaluation_report,\n",
    "#                 json_path=\"performance_metrics.rmse.value\" ## evaluation.py에서 json으로 performance를 기록한 대로 한다. \n",
    "#                                                            ## 즉, S3에 저장된 evaluation-<model_name>.json 파일안에 있는 값을 적어줘야 한다. \n",
    "#             ),\n",
    "#             right=self.args.config.get_value(\"CONDITION\", \"thesh_rmse\", dtype=\"float\"),\n",
    "#         )\n",
    "        \n",
    "#         self.condition_process = ConditionStep(\n",
    "#             name=\"CheckCondition\",\n",
    "#             display_name=\"CheckCondition\",\n",
    "#             conditions=[self.condition_acc, self.condition_prec], ## 여러 조건 함께 사용할 수 있음\n",
    "#             if_steps=[self.register_process],\n",
    "#             else_steps=[self.fail_process],\n",
    "#             #depends_on=[self.evaluation_process]\n",
    "#         )\n",
    "        \n",
    "#         print (\"  \\n== Condition Step ==\")\n",
    "#         print (\"   \\nArgs: \")\n",
    "#         for key, value in self.condition_process.arguments.items():\n",
    "#             print (\"===========================\")\n",
    "#             print (f'key: {key}')\n",
    "#             pprint (value)\n",
    "        \n",
    "    def _get_pipeline(self, ):\n",
    "        \n",
    "        pipeline = Pipeline(\n",
    "            name=self.strPipelineName,\n",
    "            steps=[self.preprocessing_process],\n",
    "            #steps=[self.preprocessing_process, self.training_process, self.evaluation_process, self.condition_process],\n",
    "            sagemaker_session=self.pipeline_session\n",
    "        )\n",
    "\n",
    "        return pipeline\n",
    "                            \n",
    "    def execution(self, ):\n",
    "         \n",
    "        self._step_preprocessing()\n",
    "        # self._step_training()\n",
    "        # self._step_evaluation()\n",
    "        # self._step_model_registration()\n",
    "        # self._step_fail()\n",
    "        # self._step_condition()\n",
    "        \n",
    "        pipeline = self._get_pipeline()\n",
    "        pipeline.upsert(role_arn=self.strExcutionRole) ## Submit the pipeline definition to the SageMaker Pipelines service \n",
    "        execution = pipeline.start()\n",
    "        desc = execution.describe()\n",
    "        \n",
    "#         self.pm.put_params(\n",
    "#             key=\"-\".join([self.strPrefix, \"PIPELINE-ARN\"]),\n",
    "#             value=desc[\"PipelineArn\"],\n",
    "#             overwrite=True\n",
    "#         )\n",
    "        #print (execution.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9088b8f8-f818-4b5d-8bba-34f86709afac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== config info. ======\n",
      "  LOCALMODE: mode:True\n",
      "  COMMON: prefix:DJ-SM-PIPELINE\n",
      "  COMMON: region:us-east-1\n",
      "  COMMON: role:arn:aws:iam::419974056037:role/service-role/AmazonSageMaker-ExecutionRole-20221206T163436\n",
      "  COMMON: bucket:sm-anomaly-detection-dongjin\n",
      "  COMMON: model_name:MODEL-1\n",
      "  COMMON: image_uri_prep:419974056037.dkr.ecr.us-east-1.amazonaws.com/mlops-image-prep:latest\n",
      "  COMMON: image_uri:419974056037.dkr.ecr.us-east-1.amazonaws.com/mlops-image-tr:latest\n",
      "  PIPELINE: enable_caching:True\n",
      "  PIPELINE: expire_after:T48H\n",
      "  PREPROCESSING: data_path:s3://sm-anomaly-detection-dongjin/data\n",
      "  PREPROCESSING: data_name:merged_clicks_1T.csv\n",
      "  PREPROCESSING: framework_version:2.0.0\n",
      "  PREPROCESSING: instance_type:ml.m5.xlarge\n",
      "  PREPROCESSING: instance_count:1\n",
      "  PREPROCESSING: shingle_size:4\n",
      "  TRAINING: framework_version:1.5-1\n",
      "  TRAINING: instance_type:ml.m5.2xlarge\n",
      "  TRAINING: instance_count:1\n",
      "  EVALUATION: framework_version:1.5-1\n",
      "  EVALUATION: instance_type:ml.m5.2xlarge\n",
      "  EVALUATION: instance_count:1\n",
      "  CONDITION: thesh_mse:0.6\n",
      "  CONDITION: thesh_rmse:0.05\n",
      "  MODEL_REGISTER: framework_version:1.5-1\n",
      "  MODEL_REGISTER: model_approval_status_default:PendingManualApproval\n",
      "  MODEL_REGISTER: inference_instances:[\"ml.m5.2xlarge\"]\n",
      "  MODEL_REGISTER: transform_instances:[\"ml.m5.2xlarge\"]\n",
      "  DEPLOY: processing_instance_type:ml.m5.xlarge\n",
      "  DEPLOY: processing_instance_count:1\n",
      "  DEPLOY: processing_framework_version:1.0-1\n",
      "  DEPLOY: instance_type:ml.m5.2xlarge\n",
      "  DEPLOY: initial_instance_count:1\n",
      "  DEPLOY: model_server_workers:1\n",
      "  DEPLOY: framework_version:2.0.0\n",
      "  DEPLOY: py_version:py310\n",
      "==========================\n",
      "Received arguments Namespace(config=<pipeline_config.config.config_handler object at 0x7efd30b56800>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " == Envrionment parameters == \n",
      "   SAGEMAKER-ROLE-ARN: arn:aws:iam::419974056037:role/service-role/AmazonSageMaker-ExecutionRole-20221206T163436\n",
      "   PREFIX: DJ-SM-PIPELINE\n",
      "   BUCKET: sm-anomaly-detection-dongjin\n",
      "   IMAGE-URI: 419974056037.dkr.ecr.us-east-1.amazonaws.com/mlops-image-tr:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "== Preprocessing Step ==\n",
      "   \n",
      "Args: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded ./src/preprocessing to s3://sagemaker-us-east-1-419974056037/preprocessing-2024-07-16-14-42-57-922/source/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-419974056037/preprocessing-2024-07-16-14-42-57-922/source/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker.processing:Uploaded ./src/preprocessing to s3://sagemaker-us-east-1-419974056037/DJ-SM-PIPELINE-MODEL-1/code/fe55da2ec222a058741fa6b3aa5934bd/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-419974056037/DJ-SM-PIPELINE-MODEL-1/code/54f0ef6bee583ff9186b762aaf572190/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.local.entities:Starting execution for pipeline DJ-SM-PIPELINE-MODEL-1. Execution ID is 41355e53-ec57-4421-9a5e-f4ee14dbc775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "key: ProcessingResources\n",
      "{'ClusterConfig': {'InstanceCount': 1,\n",
      "                   'InstanceType': 'ml.m5.xlarge',\n",
      "                   'VolumeSizeInGB': 30}}\n",
      "===========================\n",
      "key: AppSpecification\n",
      "{'ContainerArguments': ['--proc_prefix',\n",
      "                        '/opt/ml/processing/',\n",
      "                        '--shingle_size',\n",
      "                        '4',\n",
      "                        '--train_data_name',\n",
      "                        'merged_clicks_1T.csv'],\n",
      " 'ContainerEntrypoint': ['/bin/bash',\n",
      "                         '/opt/ml/processing/input/entrypoint/runproc.sh'],\n",
      " 'ImageUri': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.0.0-cpu-py310'}\n",
      "===========================\n",
      "key: RoleArn\n",
      "'arn:aws:iam::419974056037:role/service-role/AmazonSageMaker-ExecutionRole-20221206T163436'\n",
      "===========================\n",
      "key: ProcessingInputs\n",
      "[{'AppManaged': False,\n",
      "  'InputName': 'input-data',\n",
      "  'S3Input': {'LocalPath': '/opt/ml/processing/input',\n",
      "              'S3CompressionType': 'None',\n",
      "              'S3DataDistributionType': 'FullyReplicated',\n",
      "              'S3DataType': 'S3Prefix',\n",
      "              'S3InputMode': 'File',\n",
      "              'S3Uri': 's3://sm-anomaly-detection-dongjin/data'}},\n",
      " {'AppManaged': False,\n",
      "  'InputName': 'code',\n",
      "  'S3Input': {'LocalPath': '/opt/ml/processing/input/code/',\n",
      "              'S3CompressionType': 'None',\n",
      "              'S3DataDistributionType': 'FullyReplicated',\n",
      "              'S3DataType': 'S3Prefix',\n",
      "              'S3InputMode': 'File',\n",
      "              'S3Uri': 's3://sagemaker-us-east-1-419974056037/preprocessing-2024-07-16-14-42-57-922/source/sourcedir.tar.gz'}},\n",
      " {'AppManaged': False,\n",
      "  'InputName': 'entrypoint',\n",
      "  'S3Input': {'LocalPath': '/opt/ml/processing/input/entrypoint',\n",
      "              'S3CompressionType': 'None',\n",
      "              'S3DataDistributionType': 'FullyReplicated',\n",
      "              'S3DataType': 'S3Prefix',\n",
      "              'S3InputMode': 'File',\n",
      "              'S3Uri': 's3://sagemaker-us-east-1-419974056037/preprocessing-2024-07-16-14-42-57-922/source/runproc.sh'}}]\n",
      "===========================\n",
      "key: ProcessingOutputConfig\n",
      "{'Outputs': [{'AppManaged': False,\n",
      "              'OutputName': 'output-data',\n",
      "              'S3Output': {'LocalPath': '/opt/ml/processing/output',\n",
      "                           'S3UploadMode': 'EndOfJob',\n",
      "                           'S3Uri': 's3://sm-anomaly-detection-dongjin/DJ-SM-PIPELINE-MODEL-1/preprocessing/output'}}]}\n",
      "<class 'sagemaker.workflow.properties.Properties'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded ./src/preprocessing to s3://sagemaker-us-east-1-419974056037/DJ-SM-PIPELINE-MODEL-1/code/fe55da2ec222a058741fa6b3aa5934bd/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-419974056037/DJ-SM-PIPELINE-MODEL-1/code/54f0ef6bee583ff9186b762aaf572190/runproc.sh\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "INFO:sagemaker.local.entities:Starting pipeline step: 'PreprocessingProcess'\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker.local.image:'Docker Compose' is not installed. Proceeding to check for 'docker-compose' CLI.\n",
      "INFO:sagemaker.local.image:'Docker Compose' found using Docker Compose CLI.\n",
      "INFO:sagemaker.local.local_session:Starting processing job\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.local.image:No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-9ye57:\n",
      "    container_name: 6073tuhgfp-algo-1-9ye57\n",
      "    entrypoint:\n",
      "    - /bin/bash\n",
      "    - /opt/ml/processing/input/entrypoint/runproc.sh\n",
      "    - --proc_prefix\n",
      "    - /opt/ml/processing/\n",
      "    - --shingle_size\n",
      "    - '4'\n",
      "    - --train_data_name\n",
      "    - merged_clicks_1T.csv\n",
      "    environment: []\n",
      "    image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.0.0-cpu-py310\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-9ye57\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmpcsb2o3g8/algo-1-9ye57/config:/opt/ml/config\n",
      "    - /tmp/tmpcsb2o3g8/algo-1-9ye57/output:/opt/ml/output\n",
      "    - /tmp/tmpemw092ht:/opt/ml/processing/input\n",
      "    - /tmp/tmp6yi0_v08:/opt/ml/processing/input/code/\n",
      "    - /tmp/tmptfb5fiay:/opt/ml/processing/input/entrypoint\n",
      "    - /tmp/tmpp0otg5ac/output/output-data:/opt/ml/processing/output\n",
      "    - /tmp/tmpcsb2o3g8/shared:/opt/ml/shared\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmpcsb2o3g8/docker-compose.yaml up --build --abort-on-container-exit\n",
      "INFO:sagemaker.local.entities:Pipeline step 'PreprocessingProcess' FAILED. Failure message is: RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/tmpcsb2o3g8/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit']. Process exited with code: 1\n",
      "INFO:sagemaker.local.entities:Pipeline execution 41355e53-ec57-4421-9a5e-f4ee14dbc775 FAILED because step 'PreprocessingProcess' failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2024-07-16T14:42:59Z\" level=warning msg=\"/tmp/tmpcsb2o3g8/docker-compose.yaml: `version` is obsolete\"\n",
      "time=\"2024-07-16T14:42:59Z\" level=warning msg=\"a network with name sagemaker-local exists but was not created by compose.\\nSet `external: true` to use an existing network\"\n",
      "network sagemaker-local was found but has incorrect label com.docker.compose.network set to \"\"\n"
     ]
    }
   ],
   "source": [
    "# strBasePath, strCurrentDir = path.dirname(path.abspath(__file__)), os.getcwd()\n",
    "# os.chdir(strBasePath)\n",
    "# print (\"==================\")\n",
    "# print (f\"  Working Dir: {os.getcwd()}\")\n",
    "# print (f\"  You should execute 'mlops_pipeline.py' in 'pipeline' directory'\") \n",
    "# print (\"==================\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args, _ = parser.parse_known_args()\n",
    "args.config = config_handler()\n",
    "\n",
    "print(\"Received arguments {}\".format(args))\n",
    "os.environ['AWS_DEFAULT_REGION'] = args.config.get_value(\"COMMON\", \"region\")\n",
    "\n",
    "pipe_tr = mlops_pipeline(args)\n",
    "pipe_tr.execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ba7b0-f757-4336-80b7-3f2163329f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c6781d-6982-4c37-b531-8bad068c28b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker-compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d8f8a-dd7b-4034-a574-fe150b240a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
